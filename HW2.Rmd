---
title: "HW2"
author: "Michele Amoroso & Manuel Balzan"
date: "1/9/2021"
output: html_document
---


```{r}
# Handmade EM4MoG ---------------------------------------------------------

fit_em = function(vals, weights, mu, sigma, n_iter, plot_flag = T)
{
  num_gauss = length(weights)
  # Init / 2 components only
  cols     = c(rgb(1,0,0,.3), rgb(0,1,0,.3), rgb(0,0,1,.3))
  like     = 0
  for(i in 1:num_gauss){
    like = like + weights[i]*dnorm(vals, mu[i], sigma[i])
  }
  deviance = -2*sum(log(like))
  res      = matrix(NA, n_iter + 1, num_gauss*3 + 2)
  res[1,]  = c(0, weights, mu, sigma, deviance)

  for (iter in 1:n_iter) {
    d = list()
    r = list()
    
    # E step
    for(i in 1:num_gauss){
      d[i] = list(weights[i]*dnorm(vals, mu[i], sigma[i]))
    }
    for(i in 1:num_gauss){
      sum_d = 0
      for(j in 1:length(d)){
        sum_d = sum_d + d[[j]]
      }
      r[i] = list(d[[i]]/sum_d)
    }
    
    # M step
    for(i in 1:num_gauss){
      weights[i]     = mean(r[[i]])
      mu[i]    = sum(r[[i]]*vals)/sum(r[[i]])
      sigma[i] = sqrt( sum(r[[i]]*(vals^2))/sum(r[[i]]) - (mu[i])^2 )
    }
    
    # -2 x log-likelihood (a.k.a. deviance)
    like     = 0
    for(i in 1:num_gauss){
      like = like + weights[i]*dnorm(vals, mu[i], sigma[i])
    }
    deviance = -2*sum( log(like) )
    
    # Save
    res[iter+1,] = c(iter, weights, mu, sigma, deviance)
    
    #Auxiliar function
    curve_val = function(x){
      res = 0
      for(i in 1:num_gauss){
        res = res + weights[i]*dnorm(x, mu[i], sigma[i])
      }
      return(res)
    }
    
    # Plot
    if(iter == 300){
      if (plot_flag){
        hist(vals, prob = T, breaks = 30, col = gray(.8), border = NA, 
             main = "", xlab = paste("EM Iteration: ", iter, "/", n_iter, sep = ""), ylim = c(0, 0.6))
        set.seed(123)
        points(jitter(vals), rep(0,length(vals)), 
               pch = 19, cex = .6, 
               col = cols[ (dnorm(vals,mu[1],sigma[1]) > dnorm(vals,mu[2],sigma[2])) + 1])
        curve(curve_val, lwd = 4, col = rgb(0,0,0,.5), add = TRUE)
        #Sys.sleep(0.1)
      }
    }
  }
  
  res = data.frame(res)
  names(res) = c("iteration", paste("p", 1:num_gauss, sep=""), paste("mu", 1:num_gauss, sep=""), paste("sigma", 1:num_gauss, sep=""), "deviance") # generalize the number of columns with k components parameters
  out = list(parameters = c(weights = weights, mu = mu, sigma = sigma), deviance = deviance, res = res) # return the final result
  return(out)
}

data("faithful")
?faithful
hem_fit = fit_em(faithful$waiting, 
                       weights      = c(.3,.3,.3), 
                       mu     = c(40,55,70), 
                       sigma  = c(8,8,8), 
                       n_iter = 20)
round( hem_fit$parameters, 3 )
hem_fit$deviance
```

```{r}
require(pracma)

kmpp = function(vals, k) {
  centroids = matrix(0, nrow= k, ncol = 1)
  n = nrow(vals)
  centroids[1] = sample(1:n, 1)
  
  for (i in 2:k) {
    distances = distmat(vals, matrix(vals[centroids, ]))
    pr = apply(distances, 1, min); pr[centroids] = 0
    centroids[i] = sample(1:n, 1, prob = pr)
  }
  
  return(list(centers = vals[centroids]))
  #kmeans(vals, vals[centroids, ])
}

```

```{r}

initialize_parameters = function(vals, k){
    
  mat = matrix(nrow = length(vals), ncol = 1)
  mat[, 1] = vals
  kpp = kmpp(mat, k)
  
  weights = rep(1/k, k)
  mu = c(kpp$centers)
  
  if(k == 1){
    mu = mu[1]
  }
  sigma = rep(0.2, k)
  
  return(list(vals = vals, weights = weights, mu = mu, sigma = sigma))
}
```


```{r}
require(mixtools)
sample_and_fit = function(sample_size, gauss_num){
  my_sample = rnormmix(sample_size,
               lambda = c(0.5, rep(0.1,5)),
               mu = c(0, ((0:4)/2)-1),
               sigma = c(1, rep(0.1,5)) )
  
  par = initialize_parameters(my_sample, gauss_num)
  weights = par$weights
  mu = par$mu
  sigma = par$sigma

  out1 = fit_em(my_sample, weights, mu, sigma, n_iter = 301)
  r = out1$res
  print(r)
  #print(r[length(r)])
}
```

```{r}
sample_and_fit(sample_size = 250, gauss_num = 6)
```

```{r}
sample_and_fit(sample_size = 3000, gauss_num = 6)
```

```{r}
k_max = 12
weights_list = list()
mu_list = list()
sigma_list = list()

n = 200
vals = rnormmix(n,
             lambda = c(0.5, rep(0.1,5)),
             mu = c(0, ((0:4)/2)-1),
             sigma = c(1, rep(0.1,5)) )

for(k in 1:k_max){
  par = initialize_parameters(vals = vals, k = k)
  weights_list = append(weights_list, list(par$weights))
  mu_list = append(mu_list, list(par$mu))
  sigma_list = append(sigma_list, list(par$sigma))
}
```



```{r}
# AIC

AIC = function(y, kmax, n_iter, weights_list, mu_list, sigma_list) {
  aic.j = c()
  
  for (k in 1:kmax){
    weights = weights_list[[k]]
    mu = mu_list[[k]]
    sigma = sigma_list[[k]]
    
    out.opt = fit_em(y, weights, mu, sigma, n_iter = n_iter, plot_flag = F)
    
    weights = out.opt$parameters[1:k]
    mu = out.opt$parameters[(k+1):(2*k)]
    sigma = out.opt$parameters[(2*k+1):(3*k)]
    
    like = 0
    for(i in 1:k){
      like = like + weights[i]*dnorm(y, mu[i], sigma[i])
    }

    aic.j = c(aic.j, 2 * sum(log(like)) - 2 * k)
  }

  best.k = which.max(aic.j) 
  return(best.k)
}
```


```{r}
AIC(y = vals, kmax = 8, n_iter = 20, weights_list = weights_list, mu_list = mu_list, sigma_list = sigma_list)
```



```{r}
BIC = function(y, kmax, n_iter, weights_list, mu_list, sigma_list) {
  bic.j = c()
  len.y = length(y)

  for (k in 1:kmax){
    weights = weights_list[[k]]
    mu = mu_list[[k]]
    sigma = sigma_list[[k]]

    out.opt = fit_em(y, weights, mu, sigma, n_iter = n_iter, plot_flag = F)

    weights = out.opt$parameters[1:k]
    mu = out.opt$parameters[(k+1):(2*k)]
    sigma = out.opt$parameters[(2*k+1):(3*k)]

    like = 0
    for(i in 1:k){
      like = like + weights[i]*dnorm(y, mu[i], sigma[i])
    }
  
    bic.j = c(bic.j, sum(log(like)) - log(len.y)/2 * k)
  }

  best.k = which.max(bic.j) 
  return(best.k)
}
```



```{r}
BIC(y = vals, kmax = 8, n_iter = 20, weights_list = weights_list, mu_list = mu_list, sigma_list = sigma_list)
```


```{r}
training.model = function(y_train, y_test, kmax, n_iter, weights_list, mu_list, sigma_list) {
  like.j = c()
  
  for (k in 1:kmax){
    weights = weights_list[[k]]
    mu = mu_list[[k]]
    sigma = sigma_list[[k]]
    
    out.opt = fit_em(y_train, weights, mu, sigma, n_iter = n_iter, plot_flag = F) # get the estimated parameters
    
    weights = out.opt$parameters[1:k]
    mu = out.opt$parameters[(k+1):(2*k)]
    sigma = out.opt$parameters[(2*k+1):(3*k)]
    
    like = 0
    for(i in 1:k){
      like = like + weights[i]*dnorm(y_test, mu[i], sigma[i])
    }
    
    like.j = c(like.j, sum(log(like)))
  }
  
  best.k = which.max(like.j) # return the index of the k component the maximize the score 
  return(best.k)
}
```


```{r}
require(caret)
# 50% 50%
trainIndex = createDataPartition(vals, p = .5, 
                                  list = FALSE, 
                                  times = 1)
y_train = vals[ trainIndex]
y_test  = vals[-trainIndex]

training.model(y_train, y_test, kmax = 8, n_iter = 20, weights_list, mu_list, sigma_list)
```



```{r}
# 70% 30%
trainIndex = createDataPartition(vals, p = .7, 
                                  list = FALSE, 
                                  times = 1)
y_train = vals[ trainIndex]
y_test  = vals[-trainIndex]

training.model(y_train, y_test, kmax = 8, n_iter = 20, weights_list, mu_list, sigma_list)
```



```{r}
# 30% 70%
trainIndex = createDataPartition(vals, p = .3, 
                                  list = FALSE, 
                                  times = 1)
y_train = vals[ trainIndex]
y_test  = vals[-trainIndex]

training.model(y_train, y_test, kmax = 8, n_iter = 20, weights_list, mu_list, sigma_list)
```

```{r message=FALSE, warning=FALSE}
library(caret)
k.fold.cross.validation = function(y, k_folds, kmax, n_iter, weights_list, mu_list, sigma_list) {
  best.cross.v = c() 
  expectations = c()
  like.j = c()
  
  lst.k.folds = createFolds(y, k = k_folds, list = TRUE, returnTrain = FALSE) # create k folds with n observations
  
  fold.names = names(lst.k.folds)
  for (k in 1:kmax) { # train the model kmax iterations
    for (x in 1:k_folds) { # train each fold k times
      # divide the observations
      n.k.fold = fold.names[x]
      
      test.set = y[unlist(lst.k.folds[n.k.fold], use.names=FALSE)]            
      train.set = y[-unlist(lst.k.folds[n.k.fold], use.names = FALSE)]          
      
      # randomly initialize parameters
      weights = weights_list[[k]]
      mu = mu_list[[k]]
      sigma = sigma_list[[k]]
      
      # get the estimated parameters
      out.opt = fit_em(train.set, weights, mu, sigma, n_iter = n_iter, plot_flag = F)
      
      weights = out.opt$parameters[1:k]
      mu = out.opt$parameters[(k+1):(2*k)]
      sigma = out.opt$parameters[(2*k+1):(3*k)]
      
      # calculate the likelihood with the previous estimated parameters
      like = 0
      for(i in 1:k){
        like = like + weights[i]*dnorm(y_test, mu[i], sigma[i])
      }
      
      # for each fold calculate the mean of the log-likelihood
      like.j = c(like.j, mean(log(like)))
    }
    
    # save the i-th expectation
    expectations = c(expectations, mean(like.j))
  }
  best.cross.v = which.max(expectations) # return the index of the k component the maximize the score
  return(best.cross.v)
}
```


```{r}
k.fold.cross.validation(vals, k_folds = 5, kmax = 8, n_iter = 20, weights_list, mu_list, sigma_list)
```

```{r}
k.fold.cross.validation(vals, k_folds = 10, kmax = 8, n_iter = 20, weights_list, mu_list, sigma_list)
```

```{r message=FALSE, warning=FALSE}
library(KScorrect) # import this library in order to apply the integrate function
wass.score <- function(y_train, y_test, kmax, n_iter, weights_list, mu_list, sigma_list) { 
  f <- function(z, p, mu, sigma, y_test) {
    res <- abs(qmixnorm(p = z, mean = mu, sd = sigma, pro = p) - quantile(y_test, probs = z))
    return(res)
  }
  
  wass.sc <- c();
  
  for (k in 1:kmax){
    weights = weights_list[[k]]
    mu = mu_list[[k]]
    sigma = sigma_list[[k]]
    
    out.opt <- fit_em(y_train, weights, mu, sigma, n_iter = n_iter, plot_flag = F) # get the estimated
    
    weights <- out.opt$parameters[1:k]
    mu <- out.opt$parameters[(k+1):(2*k)]
    sigma <- out.opt$parameters[(2*k+1):(3*k)]
    
    wass.sc <- c(wass.sc, integrate(f, lower = 0, upper = 1,
                                    p = weights, mu = mu, sigma = sigma, y_test = y_test, 
                                    rel.tol=.Machine$double.eps^.05)$value) # execute the integral in order to have the score
  }
  
  best.wass <- wass.sc # return the index of the k component the minimize the score
  return(best.wass)
}
```

```{r}
# 50% 50%
trainIndex = createDataPartition(vals, p = .5, 
                                  list = FALSE, 
                                  times = 1)
y_train = vals[ trainIndex]
y_test  = vals[-trainIndex]

wass.score(y_train, y_test, kmax = 8, n_iter = 25, weights_list, mu_list, sigma_list)
```


```{r}
sample_from_bart <- function(k_max, n){
  
  weights_list = list()
  mu_list = list()
  sigma_list = list()
  
  vals = rnormmix(n,
               lambda = c(0.5, rep(0.1,5)),
               mu = c(0, ((0:4)/2)-1),
               sigma = c(1, rep(0.1,5)) )
  
  for(k in 1:k_max){
    par = initialize_parameters(vals = vals, k = k)
    weights_list = append(weights_list, list(par$weights))
    mu_list = append(mu_list, list(par$mu))
    sigma_list = append(sigma_list, list(par$sigma))
  }
  
  return(list(vals = vals, weights_list = weights_list, mu_list = mu_list, sigma_list = sigma_list))
}
```


```{r}
simulate <- function(sample_size, M, n_iter, kmax){
  aic = c()
  bic = c()
  sample_splitting_50_50 = c()
  sample_splitting_70_30 = c()
  sample_splitting_30_70 = c()
  cross_validation_5 = c()
  cross_validation_10 = c()
  wasserstein = c()
  
  for(i in 1:M){
    
    par = sample_from_bart(k_max = kmax, n = sample_size)
    vals = par$vals
    weights_list = par$weights_list
    mu_list = par$mu_list
    sigma_list = par$sigma_list
    
    aic_val = AIC(y = vals, kmax = kmax, n_iter = n_iter, weights_list = weights_list, mu_list = mu_list, sigma_list = sigma_list)
    aic = c(aic, aic_val)
    
    
    bic_val = BIC(y = vals, kmax = kmax, n_iter = n_iter, weights_list = par$weights_list, mu_list = par$mu_list, sigma_list = par$sigma_list)
    bic = c(bic, bic_val)
    
    
    # 50% 50%
    trainIndex = createDataPartition(vals, p = .5, 
                                      list = FALSE, 
                                      times = 1)
    y_train = vals[ trainIndex]
    y_test  = vals[-trainIndex]
    sample_splitting_50_50_val = training.model(y_train, y_test, kmax = k_max, n_iter = n_iter, weights_list, mu_list, sigma_list)
    sample_splitting_50_50 = c(sample_splitting_50_50, sample_splitting_50_50_val)
    

    # 70% 30%
    trainIndex = createDataPartition(vals, p = .7, 
                                      list = FALSE, 
                                      times = 1)
    y_train = vals[ trainIndex]
    y_test  = vals[-trainIndex]
    sample_splitting_70_30_val = training.model(y_train, y_test, kmax = k_max, n_iter = n_iter, weights_list, mu_list, sigma_list)
    sample_splitting_70_30 = c(sample_splitting_70_30, sample_splitting_70_30_val)
    

    # 30% 70%
    trainIndex = createDataPartition(vals, p = .3, 
                                      list = FALSE, 
                                      times = 1)
    y_train = vals[ trainIndex]
    y_test  = vals[-trainIndex]
    sample_splitting_30_70_val = training.model(y_train, y_test, kmax = k_max, n_iter = n_iter, weights_list, mu_list, sigma_list)
    sample_splitting_30_70 = c(sample_splitting_30_70, sample_splitting_30_70_val)
    

    
    cross_validation_5_val = k.fold.cross.validation(vals, k_folds = 5, kmax = kmax, n_iter = n_iter, weights_list, mu_list, sigma_list)
    cross_validation_5 = c(cross_validation_5, cross_validation_5_val)
    
    cross_validation_10_val = k.fold.cross.validation(vals, k_folds = 10, kmax = kmax, n_iter = n_iter, weights_list, mu_list, sigma_list)
    cross_validation_10 = c(cross_validation_10, cross_validation_10_val)
  
  }
  
  print("====== aic ======")
  print(aic)
  print("====== bic ======")
  print(bic)
  print("====== sample_splitting_50_50 ======")
  print(sample_splitting_50_50)
  print("====== sample_splitting_70_30 ======")
  print(sample_splitting_70_30)
  print("====== sample_splitting_30_70 ======")
  print(sample_splitting_30_70)
  print("====== cross_validation_5 ======")
  print(cross_validation_5)
  print("====== cross_validation_10 ======")
  print(cross_validation_10)
}
```



```{r}
simulate(sample_size = 200, M = 50, n_iter = 100, kmax = 12)
```

```{r}
simulate(sample_size = 2000, M = 50, n_iter = 100, kmax = 12)
```








