---
title: "HW2"
author: "Michele Amoroso & Manuel Balzan"
date: "1/9/2021"
output: html_document
---

For this homework we want to learn the Bart distribution from data using a MoG. To do so, we'll design a simulation study and compare the performance of different model selection techniques.

Our starting point is the following function which implements an expectation-maximization (EM) algorithm. Given some input data, this iterative algorithm computes (local) maximum likelihood estimates using a MoG model. 

```{r}

fit_em = function(vals, weights, mu, sigma, n_iter, plot_flag = T)
{
  num_gauss = length(weights)
  # Init / 2 components only
  cols     = c(rgb(1,0,0,.3), rgb(0,1,0,.3), rgb(0,0,1,.3))
  like     = 0
  for(i in 1:num_gauss){
    like = like + weights[i]*dnorm(vals, mu[i], sigma[i])
  }
  deviance = -2*sum(log(like))
  res      = matrix(NA, n_iter + 1, num_gauss*3 + 2)
  res[1,]  = c(0, weights, mu, sigma, deviance)

  for (iter in 1:n_iter) {
    d = list()
    r = list()
    
    # E step
    for(i in 1:num_gauss){
      d[i] = list(weights[i]*dnorm(vals, mu[i], sigma[i]))
    }
    for(i in 1:num_gauss){
      sum_d = 0
      for(j in 1:length(d)){
        sum_d = sum_d + d[[j]]
      }
      r[i] = list(d[[i]]/sum_d)
    }
    
    # M step
    for(i in 1:num_gauss){
      weights[i]     = mean(r[[i]])
      mu[i]    = sum(r[[i]]*vals)/sum(r[[i]])
      sigma[i] = sqrt( sum(r[[i]]*(vals^2))/sum(r[[i]]) - (mu[i])^2 )
    }
    
    
    # -2 x log-likelihood (a.k.a. deviance)
    like     = 0
    for(i in 1:num_gauss){
      like = like + weights[i]*dnorm(vals, mu[i], sigma[i])
    }
    deviance = -2*sum( log(like) )
    
    # Save
    res[iter+1,] = c(iter, weights, mu, sigma, deviance)
    
    #Auxiliar function
    curve_val = function(x){
      res = 0
      for(i in 1:num_gauss){
        res = res + weights[i]*dnorm(x, mu[i], sigma[i])
      }
      return(res)
    }
    
    # Plot
    if(iter == 300){
      if (plot_flag){
        hist(vals, prob = T, breaks = 30, col = gray(.8), border = NA, 
             main = "", xlab = paste("EM Iteration: ", iter, "/", n_iter, sep = ""), ylim = c(0, 0.6))
        set.seed(123)
        points(jitter(vals), rep(0,length(vals)), 
               pch = 19, cex = .6, 
               col = cols[ (dnorm(vals,mu[1],sigma[1]) > dnorm(vals,mu[2],sigma[2])) + 1])
        curve(curve_val, lwd = 4, col = rgb(0,0,0,.5), add = TRUE)
        #Sys.sleep(0.1)
      }
    }
  }
  
  res = data.frame(res)
  names(res) = c("iteration", paste("p", 1:num_gauss, sep=""), paste("mu", 1:num_gauss, sep=""), paste("sigma", 1:num_gauss, sep=""), "deviance") # generalize the number of columns with k components parameters
  out = list(parameters = c(weights = weights, mu = mu, sigma = sigma), deviance = deviance, res = res) # return the final result
  return(out)
}

data("faithful")
?faithful
hem_fit = fit_em(faithful$waiting, 
                       weights      = c(.3,.3,.3), 
                       mu     = c(40,55,70), 
                       sigma  = c(8,8,8), 
                       n_iter = 20)
round( hem_fit$parameters, 3 )
hem_fit$deviance
```

In order to help the EM algorithm find a good solution, we want to start from some reasonable mu parameters instead of choosing completely random ones.
We decided to use the kmeans++ inizialization to select the first mu parameters.
The main issue with this approach is that it can't identify the wider central Gaussian (the one with weight: 0.5, mu: 0 and sigma: 1) that models the Bart tails.
As a result, our model will probably use some extra Gaussians for the tails and the optimal number of Gaussians won't be exactly 6.

```{r}
require(pracma)

kmpp = function(vals, k) {
  centroids = matrix(0, nrow= k, ncol = 1)
  n = nrow(vals)
  centroids[1] = sample(1:n, 1)
  
  for (i in 2:k) {
    distances = distmat(vals, matrix(vals[centroids, ]))
    pr = apply(distances, 1, min); pr[centroids] = 0
    centroids[i] = sample(1:n, 1, prob = pr)
  }
  
  return(list(centers = vals[centroids]))
}

```

We also want to initialize the weights and sigmas of our Gaussians.
To do so, we assign an equal weight for each Gaussian (they sum up to 1) and a sigma of 0.2 for each of them.

The following function combines the mu initialization with the weights and sigmas initialization.

```{r}

initialize_parameters = function(vals, k){
    
  mat = matrix(nrow = length(vals), ncol = 1)
  mat[, 1] = vals
  kpp = kmpp(mat, k)
  
  weights = rep(1/k, k)
  mu = c(kpp$centers)
  
  if(k == 1){
    mu = mu[1]
  }
  sigma = rep(0.2, k)
  
  return(list(vals = vals, weights = weights, mu = mu, sigma = sigma))
}
```

We define a function that samples from the Bart and initializes the parameters for each number of Guassians.

```{r}
sample_from_bart <- function(k_max, n){
  
  weights_list = list()
  mu_list = list()
  sigma_list = list()
  
  vals = rnormmix(n,
               lambda = c(0.5, rep(0.1,5)),
               mu = c(0, ((0:4)/2)-1),
               sigma = c(1, rep(0.1,5)) )
  
  for(k in 1:k_max){
    par = initialize_parameters(vals = vals, k = k)
    weights_list = append(weights_list, list(par$weights))
    mu_list = append(mu_list, list(par$mu))
    sigma_list = append(sigma_list, list(par$sigma))
  }
  
  return(list(vals = vals, weights_list = weights_list, mu_list = mu_list, sigma_list = sigma_list))
}
```

we now want to test our EM function with a specific number of Gaussians (we pick 6 in this case because it's the true number of Gaussians of the Bart).

We pick two sample sizes n1 < n2, to probe a clearly non-asymptotic and a reasonably asymptotic regime.
In our case n1 = 200 and n2 = 2000.

```{r}
require(mixtools)
sample_and_fit = function(sample_size, gauss_num){
  my_sample = rnormmix(sample_size,
               lambda = c(0.5, rep(0.1,5)),
               mu = c(0, ((0:4)/2)-1),
               sigma = c(1, rep(0.1,5)) )
  
  par = initialize_parameters(my_sample, gauss_num)
  weights = par$weights
  mu = par$mu
  sigma = par$sigma

  out1 = fit_em(my_sample, weights, mu, sigma, n_iter = 301)
  r = out1$res
  print(r)
  #print(r[length(r)])
}
```

Non asymptotic Regime.

```{r}
sample_and_fit(sample_size = 200, gauss_num = 6)
```

Asymptotic Regime.

```{r}
sample_and_fit(sample_size = 2000, gauss_num = 6)
```

3a. AIC

Our first model selection method is the Akaike Information Criterion (AIC).

Given a collection of models with different number of Gaussians, this method estimates the quality of each model and picks the one with the highest score.

In particular, AIC rewards goodness of fit (as assessed by the likelihood function), but it also includes a penalty that is an increasing function of the number of estimated parameters.

This allows to prevent overfitting.

```{r}
# AIC

AIC = function(y, kmax, n_iter, weights_list, mu_list, sigma_list) {
  aic = c()
  
  for (k in 1:kmax){
    weights = weights_list[[k]]
    mu = mu_list[[k]]
    sigma = sigma_list[[k]]
    
    res = fit_em(y, weights, mu, sigma, n_iter = n_iter, plot_flag = F)
    
    weights = res$parameters[1:k]
    mu = res$parameters[(k+1):(2*k)]
    sigma = res$parameters[(2*k+1):(3*k)]
    
    like = 0
    for(i in 1:k){
      like = like + weights[i]*dnorm(y, mu[i], sigma[i])
    }

    aic = c(aic, 2 * sum(log(like)) - 2 * (3*k-1))
  }

  best_k = which.max(aic) 
  return(best_k)
}
```

3b. BIC

A similar approach to the AIC.

```{r}
BIC = function(y, kmax, n_iter, weights_list, mu_list, sigma_list) {
  bic = c()
  y_len = length(y)

  for (k in 1:kmax){
    weights = weights_list[[k]]
    mu = mu_list[[k]]
    sigma = sigma_list[[k]]

    res = fit_em(y, weights, mu, sigma, n_iter = n_iter, plot_flag = F)

    weights = res$parameters[1:k]
    mu = res$parameters[(k+1):(2*k)]
    sigma = res$parameters[(2*k+1):(3*k)]

    like = 0
    for(i in 1:k){
      like = like + weights[i]*dnorm(y, mu[i], sigma[i])
    }
  
    bic <- c(bic, sum(log(like)) - log(y_len)/2 *(3*k-1))
  }

  best_k = which.max(bic) 
  return(best_k)
}
```

3c-d-e. Sample splitting

Now we want to split our sample in 2 different partitions: training set and test set.

The first set will be used to train the model and find the parameters, then we'll compute the likelihood on the test set and choose the model with the highest likelihood.
This approach helps to avoid overfitting, because the model is tested on data that hasn't been seen before.


```{r}
sample_splitting = function(y_train, y_test, kmax, n_iter, weights_list, mu_list, sigma_list) {
  like_vec = c()
  
  for (k in 1:kmax){
    weights = weights_list[[k]]
    mu = mu_list[[k]]
    sigma = sigma_list[[k]]
    
    res = fit_em(y_train, weights, mu, sigma, n_iter = n_iter, plot_flag = F) # get the estimated parameters
    
    weights = res$parameters[1:k]
    mu = res$parameters[(k+1):(2*k)]
    sigma = res$parameters[(2*k+1):(3*k)]
    
    like = 0
    for(i in 1:k){
      like = like + weights[i]*dnorm(y_test, mu[i], sigma[i])
    }
    
    like_vec = c(like_vec, sum(log(like)))
  }
  
  best_k = which.max(like_vec) # return the index of the k component the maximize the score 
  return(best_k)
}
```

3.f-e K-Fold Cross Validation

Cross Validation is another method used to assess how well a model will work on unseen data.

It works by repeatedly splitting the data in training and test set, performing the analysis on the first set and validating the analysis on the second one.
The validation results are then averaged over the rounds to give an estimate of the model's predictive performance.

With this method, we select the model with the best Cross Validation score.

```{r message=FALSE, warning=FALSE}
library(caret)
cross_validation = function(y, k_folds, kmax, n_iter, weights_list, mu_list, sigma_list) {
  expectations = c()
  like_vec = c()
  
  folds = createFolds(y, k = k_folds, list = TRUE, returnTrain = FALSE) # create k folds with n observations
  
  fold_names = names(folds)
  for (k in 1:kmax) { # train the model kmax iterations
    for (x in 1:k_folds) { # train each fold k times
      # divide the observations
      fold = fold_names[x]
      
      testIndex = unlist(folds[fold], use.names=FALSE)
      test.set = y[testIndex]            
      train.set = y[-testIndex]          
      
      # randomly initialize parameters
      weights = weights_list[[k]]
      mu = mu_list[[k]]
      sigma = sigma_list[[k]]
      
      # get the estimated parameters
      res = fit_em(train.set, weights, mu, sigma, n_iter = n_iter, plot_flag = F)
      
      weights = res$parameters[1:k]
      mu = res$parameters[(k+1):(2*k)]
      sigma = res$parameters[(2*k+1):(3*k)]
      
      # calculate the likelihood with the previous estimated parameters
      like = 0
      for(i in 1:k){
        like = like + weights[i]*dnorm(y_test, mu[i], sigma[i])
      }
      
      # for each fold calculate the mean of the log-likelihood
      like_vec = c(like_vec, mean(log(like)))
    }
    
    # save the i-th expectation
    expectations = c(expectations, mean(like_vec))
  }
  best_k = which.max(expectations) # return the index of the k component the maximize the score
  return(best_k)
}
```

Insert Wasserstein Comment.

```{r message=FALSE, warning=FALSE}
library(KScorrect) # import this library in order to apply the integrate function
wasserstein_score <- function(y_train, y_test, kmax, n_iter, weights_list, mu_list, sigma_list) { 
  f <- function(z, p, mu, sigma, y_test) {
    res <- abs(qmixnorm(p = z, mean = mu, sd = sigma, pro = p) - quantile(y_test, probs = z))
    return(res)
  }
  
  wass <- c();
  
  for (k in 1:kmax){
    weights = weights_list[[k]]
    mu = mu_list[[k]]
    sigma = sigma_list[[k]]
    
    res <- fit_em(y_train, weights, mu, sigma, n_iter = n_iter, plot_flag = F) # get the estimated
    
    weights <- res$parameters[1:k]
    mu <- res$parameters[(k+1):(2*k)]
    sigma <- res$parameters[(2*k+1):(3*k)]
    
    int <- tryCatch(integrate(f, lower = 0, upper = 1,
                        p = weights, mu = mu, sigma = sigma, y_test = y_test, 
                        rel.tol=.Machine$double.eps^.05)$value,
                    error = function(e) return(1))
    
    wass <- c(wass, int) # execute the integral in order to have the score
  }
  
  best_k <- wass # return the index of the k component the minimize the score
  return(which.min(best_k))
}
```

Now that we have defined all our model selection methods, we want to simulate M times from the Bart, for each sample size, and apply each of our model selection methods.
For each method, we will have M different prediction of the best number of Gaussians, and we will pick the most frequent one (following a frequentist approach).

Let's define the simulation function.

```{r}
simulate <- function(sample_size, M, n_iter, kmax){
  aic = c()
  bic = c()
  sample_splitting_50_50 = c()
  sample_splitting_70_30 = c()
  sample_splitting_30_70 = c()
  cross_validation_5 = c()
  cross_validation_10 = c()
  wasserstein = c()
  
  for(i in 1:M){
    
    par = sample_from_bart(k_max = kmax, n = sample_size)
    vals = par$vals
    weights_list = par$weights_list
    mu_list = par$mu_list
    sigma_list = par$sigma_list
    
    aic_val = AIC(y = vals, kmax = kmax, n_iter = n_iter, weights_list = weights_list, mu_list = mu_list, sigma_list = sigma_list)
    aic = c(aic, aic_val)
    
    
    bic_val = BIC(y = vals, kmax = kmax, n_iter = n_iter, weights_list = par$weights_list, mu_list = par$mu_list, sigma_list = par$sigma_list)
    bic = c(bic, bic_val)
    
    
    # 50% 50%
    trainIndex = createDataPartition(vals, p = .5, 
                                      list = FALSE, 
                                      times = 1)
    y_train = vals[ trainIndex]
    y_test  = vals[-trainIndex]
    sample_splitting_50_50_val = sample_splitting(y_train, y_test, kmax = k_max, n_iter = n_iter, weights_list, mu_list, sigma_list)
    sample_splitting_50_50 = c(sample_splitting_50_50, sample_splitting_50_50_val)
    

    # 70% 30%
    trainIndex = createDataPartition(vals, p = .7, 
                                      list = FALSE, 
                                      times = 1)
    y_train = vals[ trainIndex]
    y_test  = vals[-trainIndex]
    sample_splitting_70_30_val = sample_splitting(y_train, y_test, kmax = k_max, n_iter = n_iter, weights_list, mu_list, sigma_list)
    sample_splitting_70_30 = c(sample_splitting_70_30, sample_splitting_70_30_val)
    

    # 30% 70%
    trainIndex = createDataPartition(vals, p = .3, 
                                      list = FALSE, 
                                      times = 1)
    y_train = vals[ trainIndex]
    y_test  = vals[-trainIndex]
    sample_splitting_30_70_val = sample_splitting(y_train, y_test, kmax = k_max, n_iter = n_iter, weights_list, mu_list, sigma_list)
    sample_splitting_30_70 = c(sample_splitting_30_70, sample_splitting_30_70_val)
    

    
    cross_validation_5_val = cross_validation(vals, k_folds = 5, kmax = kmax, n_iter = n_iter, weights_list, mu_list, sigma_list)
    cross_validation_5 = c(cross_validation_5, cross_validation_5_val)
    
    cross_validation_10_val = cross_validation(vals, k_folds = 10, kmax = kmax, n_iter = n_iter, weights_list, mu_list, sigma_list)
    cross_validation_10 = c(cross_validation_10, cross_validation_10_val)
    
    # wasserstein
    trainIndex = createDataPartition(vals, p = .5, 
                                      list = FALSE, 
                                      times = 1)
    y_train = vals[ trainIndex]
    y_test  = vals[-trainIndex]
    wasserstein_val = wasserstein_score(y_train, y_test, kmax, n_iter, weights_list, mu_list, sigma_list)
    wasserstein = c(wasserstein, wasserstein_val)
    
    
    cat(paste(i, "| "))
    
  }
  
  best_aic = which.max(tabulate(aic))
  best_bic = which.max(tabulate(bic))
  best_sample_splitting_50_50 = which.max(tabulate(sample_splitting_50_50))
  best_sample_splitting_70_30 = which.max(tabulate(sample_splitting_70_30))
  best_sample_splitting_30_70 = which.max(tabulate(sample_splitting_30_70))
  best_cross_validation_5 = which.max(tabulate(cross_validation_5))
  best_cross_validation_10 = which.max(tabulate(cross_validation_10))
  best_wasserstein = which.max(tabulate(wasserstein))
  
  return(list(aic = aic, bic = bic, sample_splitting_50_50 = sample_splitting_50_50, sample_splitting_70_30 = sample_splitting_70_30, sample_splitting_30_70 = sample_splitting_30_70, cross_validation_5 = cross_validation_5, cross_validation_10 = cross_validation_10, wasserstein = wasserstein))
}
```

We first run it on the smaller sample size.

```{r message=FALSE, warning=FALSE}
 par_n1 = simulate(sample_size = 200, M = 100, n_iter = 25, kmax = 12)
```

Then on the bigger one.

```{r message=FALSE, warning=FALSE}
par_n2 = simulate(sample_size = 2000, M = 100, n_iter = 25, kmax = 12)
```

We are now interested in analyzing our results. Let's plot for each method, the number of occurrence of each number of Gaussians.

```{r}
plot_functions <- function(par_n){
  
  aic = table(par_n$aic)
  bic = table(par_n$bic)
  sample_splitting_50_50 = table(par_n$sample_splitting_50_50)
  sample_splitting_70_30 = table(par_n$sample_splitting_70_30)
  sample_splitting_30_70 = table(par_n$sample_splitting_30_70)
  cross_validation_5 = table(par_n$cross_validation_5)
  cross_validation_10 = table(par_n$cross_validation_10)
  wasserstein = table(par_n$wasserstein)
  
  barplot(aic, main="AIC", xlab="k")
  barplot(bic, main="BIC", xlab="k")
  barplot(sample_splitting_50_50, main="Sample Splitting with 50% in train and 50% in test", xlab="k")
  barplot(sample_splitting_70_30, main="Sample Splitting with 70% in train and 30% in test", xlab="k")
  barplot(sample_splitting_30_70, main="Sample Splitting with 30% in train and 70% in test", xlab="k")
  barplot(cross_validation_5, main="5-fold Cross-Validation", xlab="k")
  barplot(cross_validation_10, main="10-fold Cross-Validation", xlab="k")
  barplot(wasserstein, main="Wasserstein", xlab="k")
  
}
```

We first plot the smaller sample size.

```{r}
plot_functions(par_n1)
```

Then the bigger one.

```{r}
plot_functions(par_n2)
```





